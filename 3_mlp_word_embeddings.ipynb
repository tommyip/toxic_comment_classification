{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP classifier with word embeddings representation\n",
    "\n",
    "In the previous attempts, the comments text has been represented using a bag-of-words model. We were able to attain a respectable score, however, it is clear that it has trouble generalizing to new data. Words that brings toxicity are often times made up on the spot, so how ever big our token count dictionary is it can't cover the test set appropriately. Adding N-grams does not improve the model, which is disappointing. My theory is that toxic words are usually concatenated to a single token (eg _cock\\*\\*er_) so the captured n-grams (where n > 1) are just uninformative English phrases.\n",
    "\n",
    "In this notebook, we instead represent each comment as a dense word vector. We use a pre-trained model included with spaCy to vectorize each token, then we take the mean of the vectors as the document vector. Each document vector is fed into a multi-layered perceptron to predict the probabilites for each labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "from helper import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validation, x_test, y_train, y_validation, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment2vec(text):\n",
    "    doc = nlp(text)\n",
    "    # Some times a sentense consist entirely of stop words and non-alpha characters,\n",
    "    # we simply use all tokens in those cases.\n",
    "    if sum(token.is_alpha for token in doc) > 10:\n",
    "        tokens = [token.vector for token in doc if token.is_alpha]\n",
    "    else:\n",
    "        tokens = [token.vector for token in doc]\n",
    "    return np.mean(np.vstack(tokens), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus2matrix(corpus):\n",
    "    mat = np.zeros((len(corpus), 300), dtype=np.float32)\n",
    "    for vec, text in zip(mat, corpus):\n",
    "        vec[:] = comment2vec(text)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = corpus2matrix(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.58830550\n",
      "Validation score: 0.614000\n",
      "Iteration 2, loss = 2.69054374\n",
      "Validation score: 0.634000\n",
      "Iteration 3, loss = 2.13806123\n",
      "Validation score: 0.644000\n",
      "Iteration 4, loss = 1.75158131\n",
      "Validation score: 0.652000\n",
      "Iteration 5, loss = 1.53016163\n",
      "Validation score: 0.658000\n",
      "Iteration 6, loss = 1.40353446\n",
      "Validation score: 0.674000\n",
      "Iteration 7, loss = 1.32406694\n",
      "Validation score: 0.680000\n",
      "Iteration 8, loss = 1.26442988\n",
      "Validation score: 0.698000\n",
      "Iteration 9, loss = 1.21652085\n",
      "Validation score: 0.696000\n",
      "Iteration 10, loss = 1.17664764\n",
      "Validation score: 0.698000\n",
      "Iteration 11, loss = 1.14442680\n",
      "Validation score: 0.696000\n",
      "Iteration 12, loss = 1.11692232\n",
      "Validation score: 0.694000\n",
      "Iteration 13, loss = 1.09286073\n",
      "Validation score: 0.704000\n",
      "Iteration 14, loss = 1.06982721\n",
      "Validation score: 0.708000\n",
      "Iteration 15, loss = 1.05056518\n",
      "Validation score: 0.706000\n",
      "Iteration 16, loss = 1.03304410\n",
      "Validation score: 0.708000\n",
      "Iteration 17, loss = 1.01757602\n",
      "Validation score: 0.712000\n",
      "Iteration 18, loss = 1.00132046\n",
      "Validation score: 0.712000\n",
      "Iteration 19, loss = 0.98917940\n",
      "Validation score: 0.720000\n",
      "Iteration 20, loss = 0.97681181\n",
      "Validation score: 0.714000\n",
      "Iteration 21, loss = 0.96480882\n",
      "Validation score: 0.720000\n",
      "Iteration 22, loss = 0.95376871\n",
      "Validation score: 0.714000\n",
      "Iteration 23, loss = 0.94605491\n",
      "Validation score: 0.714000\n",
      "Iteration 24, loss = 0.93555380\n",
      "Validation score: 0.718000\n",
      "Iteration 25, loss = 0.92721238\n",
      "Validation score: 0.722000\n",
      "Iteration 26, loss = 0.91886569\n",
      "Validation score: 0.722000\n",
      "Iteration 27, loss = 0.91162184\n",
      "Validation score: 0.722000\n",
      "Iteration 28, loss = 0.90277148\n",
      "Validation score: 0.730000\n",
      "Iteration 29, loss = 0.89548792\n",
      "Validation score: 0.732000\n",
      "Iteration 30, loss = 0.88826227\n",
      "Validation score: 0.730000\n",
      "Iteration 31, loss = 0.88042927\n",
      "Validation score: 0.724000\n",
      "Iteration 32, loss = 0.87386338\n",
      "Validation score: 0.728000\n",
      "Iteration 33, loss = 0.86853662\n",
      "Validation score: 0.728000\n",
      "Iteration 34, loss = 0.86488375\n",
      "Validation score: 0.728000\n",
      "Iteration 35, loss = 0.86015481\n",
      "Validation score: 0.726000\n",
      "Iteration 36, loss = 0.84984333\n",
      "Validation score: 0.728000\n",
      "Iteration 37, loss = 0.84433081\n",
      "Validation score: 0.722000\n",
      "Iteration 38, loss = 0.83948503\n",
      "Validation score: 0.736000\n",
      "Iteration 39, loss = 0.83923731\n",
      "Validation score: 0.730000\n",
      "Iteration 40, loss = 0.82826865\n",
      "Validation score: 0.738000\n",
      "Iteration 41, loss = 0.82361524\n",
      "Validation score: 0.730000\n",
      "Iteration 42, loss = 0.81804862\n",
      "Validation score: 0.740000\n",
      "Iteration 43, loss = 0.81399792\n",
      "Validation score: 0.740000\n",
      "Iteration 44, loss = 0.80768902\n",
      "Validation score: 0.742000\n",
      "Iteration 45, loss = 0.80424279\n",
      "Validation score: 0.742000\n",
      "Iteration 46, loss = 0.79983501\n",
      "Validation score: 0.738000\n",
      "Iteration 47, loss = 0.79653974\n",
      "Validation score: 0.742000\n",
      "Iteration 48, loss = 0.79020580\n",
      "Validation score: 0.746000\n",
      "Iteration 49, loss = 0.78942154\n",
      "Validation score: 0.736000\n",
      "Iteration 50, loss = 0.78069442\n",
      "Validation score: 0.744000\n",
      "Iteration 51, loss = 0.77997659\n",
      "Validation score: 0.742000\n",
      "Iteration 52, loss = 0.77274889\n",
      "Validation score: 0.742000\n",
      "Iteration 53, loss = 0.76956525\n",
      "Validation score: 0.742000\n",
      "Iteration 54, loss = 0.76550438\n",
      "Validation score: 0.742000\n",
      "Iteration 55, loss = 0.76130916\n",
      "Validation score: 0.746000\n",
      "Iteration 56, loss = 0.75724179\n",
      "Validation score: 0.746000\n",
      "Iteration 57, loss = 0.75427631\n",
      "Validation score: 0.748000\n",
      "Iteration 58, loss = 0.75280468\n",
      "Validation score: 0.744000\n",
      "Iteration 59, loss = 0.74861119\n",
      "Validation score: 0.746000\n",
      "Iteration 60, loss = 0.74228396\n",
      "Validation score: 0.744000\n",
      "Iteration 61, loss = 0.73873836\n",
      "Validation score: 0.740000\n",
      "Iteration 62, loss = 0.73431881\n",
      "Validation score: 0.746000\n",
      "Iteration 63, loss = 0.73187234\n",
      "Validation score: 0.744000\n",
      "Iteration 64, loss = 0.72877184\n",
      "Validation score: 0.744000\n",
      "Iteration 65, loss = 0.72496415\n",
      "Validation score: 0.746000\n",
      "Iteration 66, loss = 0.72158829\n",
      "Validation score: 0.744000\n",
      "Iteration 67, loss = 0.71801929\n",
      "Validation score: 0.746000\n",
      "Iteration 68, loss = 0.71381335\n",
      "Validation score: 0.750000\n",
      "Iteration 69, loss = 0.71175156\n",
      "Validation score: 0.750000\n",
      "Iteration 70, loss = 0.70820890\n",
      "Validation score: 0.746000\n",
      "Iteration 71, loss = 0.70502026\n",
      "Validation score: 0.740000\n",
      "Iteration 72, loss = 0.70170610\n",
      "Validation score: 0.748000\n",
      "Iteration 73, loss = 0.69728842\n",
      "Validation score: 0.748000\n",
      "Iteration 74, loss = 0.69474137\n",
      "Validation score: 0.746000\n",
      "Iteration 75, loss = 0.69252126\n",
      "Validation score: 0.746000\n",
      "Iteration 76, loss = 0.68882460\n",
      "Validation score: 0.744000\n",
      "Iteration 77, loss = 0.68608367\n",
      "Validation score: 0.748000\n",
      "Iteration 78, loss = 0.68304026\n",
      "Validation score: 0.750000\n",
      "Iteration 79, loss = 0.68029805\n",
      "Validation score: 0.752000\n",
      "Iteration 80, loss = 0.67630233\n",
      "Validation score: 0.746000\n",
      "Iteration 81, loss = 0.67391990\n",
      "Validation score: 0.754000\n",
      "Iteration 82, loss = 0.66993349\n",
      "Validation score: 0.750000\n",
      "Iteration 83, loss = 0.66770528\n",
      "Validation score: 0.744000\n",
      "Iteration 84, loss = 0.66531967\n",
      "Validation score: 0.752000\n",
      "Iteration 85, loss = 0.66329873\n",
      "Validation score: 0.752000\n",
      "Iteration 86, loss = 0.65988958\n",
      "Validation score: 0.744000\n",
      "Iteration 87, loss = 0.65715358\n",
      "Validation score: 0.746000\n",
      "Iteration 88, loss = 0.65452412\n",
      "Validation score: 0.752000\n",
      "Iteration 89, loss = 0.65166893\n",
      "Validation score: 0.744000\n",
      "Iteration 90, loss = 0.65039978\n",
      "Validation score: 0.746000\n",
      "Iteration 91, loss = 0.64642889\n",
      "Validation score: 0.752000\n",
      "Iteration 92, loss = 0.64384700\n",
      "Validation score: 0.756000\n",
      "Iteration 93, loss = 0.64127859\n",
      "Validation score: 0.756000\n",
      "Iteration 94, loss = 0.64179075\n",
      "Validation score: 0.750000\n",
      "Iteration 95, loss = 0.63794245\n",
      "Validation score: 0.750000\n",
      "Iteration 96, loss = 0.63146406\n",
      "Validation score: 0.750000\n",
      "Iteration 97, loss = 0.63550513\n",
      "Validation score: 0.750000\n",
      "Iteration 98, loss = 0.63141401\n",
      "Validation score: 0.748000\n",
      "Iteration 99, loss = 0.62577744\n",
      "Validation score: 0.750000\n",
      "Iteration 100, loss = 0.62275581\n",
      "Validation score: 0.748000\n",
      "Iteration 101, loss = 0.62019202\n",
      "Validation score: 0.756000\n",
      "Iteration 102, loss = 0.61774395\n",
      "Validation score: 0.750000\n",
      "Iteration 103, loss = 0.61594720\n",
      "Validation score: 0.760000\n",
      "Iteration 104, loss = 0.61417368\n",
      "Validation score: 0.758000\n",
      "Iteration 105, loss = 0.61034275\n",
      "Validation score: 0.748000\n",
      "Iteration 106, loss = 0.60741605\n",
      "Validation score: 0.750000\n",
      "Iteration 107, loss = 0.60506576\n",
      "Validation score: 0.748000\n",
      "Iteration 108, loss = 0.60387275\n",
      "Validation score: 0.750000\n",
      "Iteration 109, loss = 0.60178556\n",
      "Validation score: 0.754000\n",
      "Iteration 110, loss = 0.59955578\n",
      "Validation score: 0.760000\n",
      "Iteration 111, loss = 0.59711525\n",
      "Validation score: 0.756000\n",
      "Iteration 112, loss = 0.59195242\n",
      "Validation score: 0.750000\n",
      "Iteration 113, loss = 0.59102689\n",
      "Validation score: 0.752000\n",
      "Iteration 114, loss = 0.58871211\n",
      "Validation score: 0.742000\n",
      "Iteration 115, loss = 0.58892585\n",
      "Validation score: 0.754000\n",
      "Iteration 116, loss = 0.58451957\n",
      "Validation score: 0.750000\n",
      "Iteration 117, loss = 0.58000461\n",
      "Validation score: 0.758000\n",
      "Iteration 118, loss = 0.57980113\n",
      "Validation score: 0.754000\n",
      "Iteration 119, loss = 0.57804883\n",
      "Validation score: 0.750000\n",
      "Iteration 120, loss = 0.57427644\n",
      "Validation score: 0.758000\n",
      "Iteration 121, loss = 0.57032731\n",
      "Validation score: 0.754000\n",
      "Iteration 122, loss = 0.57137038\n",
      "Validation score: 0.760000\n",
      "Iteration 123, loss = 0.56644922\n",
      "Validation score: 0.754000\n",
      "Iteration 124, loss = 0.56590366\n",
      "Validation score: 0.754000\n",
      "Iteration 125, loss = 0.56310377\n",
      "Validation score: 0.752000\n",
      "Iteration 126, loss = 0.55981343\n",
      "Validation score: 0.754000\n",
      "Iteration 127, loss = 0.55906650\n",
      "Validation score: 0.752000\n",
      "Iteration 128, loss = 0.55718656\n",
      "Validation score: 0.754000\n",
      "Iteration 129, loss = 0.55404055\n",
      "Validation score: 0.756000\n",
      "Validation score did not improve more than tol=0.000100 for 25 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),\n",
    "    random_state=42,\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=25,\n",
    "    verbose=True\n",
    ").fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our initial neural network with one hidden layer of size 100, the training score is reaching 1.0 while validation score is in the low 0.9s. This is a significant improvement over the previous Naive Bayes model, however, there also seems to be some overfitting. Deeper networks also overfitted wihout improving performance. We enabled the `early_stopping` option which stops training when validation scores stop improving to combat this problem. This also decrease training time which is a plus.\n",
    "\n",
    "The tuned model uses one hidden layer of size 50, which exhibits good generalization and validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation_vec = corpus2matrix(x_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.9861423536460044\n",
      "Validation ROC AUC: 0.9689210539389465\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict_proba(x_train_vec)\n",
    "y_validation_pred = model.predict_proba(x_validation_vec)\n",
    "\n",
    "print('Train ROC AUC:', roc_auc_score(y_train, y_train_pred))\n",
    "print('Validation ROC AUC:', roc_auc_score(y_validation, y_validation_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.9673785646821336\n"
     ]
    }
   ],
   "source": [
    "x_test_vec = corpus2matrix(x_test)\n",
    "y_test_pred = model.predict_proba(x_test_vec)\n",
    "print('Test ROC AUC:', roc_auc_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99058215, 0.95571743, 0.97939484, 0.9554448 , 0.9684104 ,\n",
       "       0.95472177])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_test_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word embeddings and neural networks improved our test scores by over 7% compared to the naive bayes model. Labels with relatively little training data also performs quite well. In the next notebook we will attempt to retrain the pretrained word vectors to improve coverage, as well as try using more sophisticated architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
